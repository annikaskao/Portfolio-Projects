---
title: "PSTAT 131 - Final Project"
author: "Annika Kao"
date: "12/5/2020"
output:
  html_document:
    df_print: paged
---
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Load Libraries 
library(knitr)
library(readr)
library(dplyr)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(maps)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(glmnet)
library(dendextend)
library(e1071)
library(scales)
library(class)
library(FNN)
library(grid)
library(gridExtra)
library(boot)
```

```{r, echo=FALSE}
# Rainbow colors for later plots and graphs 
rainbow_colors <- rainbow(7)
plot_colors <- rainbow_colors
```


DATA
```{r, warning=FALSE, message=FALSE,echo=FALSE}
## read data and convert candidate names and party names from string to factor
election.raw <- read_csv("candidates_county.csv", col_names = TRUE) %>% 
  mutate(candidate = as.factor(candidate), party = as.factor(party))

## remove the word "County" from the county names
words.to.remove = c("County")
remove.words <- function(str, words.to.remove){
  sapply(str, function(str){
    x <- unlist(strsplit(str, " "))
    x <- x[!x %in% words.to.remove]
    return(paste(x, collapse = " "))
  }, simplify = "array", USE.NAMES = FALSE)
}
election.raw$county <- remove.words(election.raw$county, words.to.remove)

## read census data
census <- read_csv("census_county.csv") 
```


[1] Election data
```{r, results = FALSE}
# Dimension of election.raw
dim(election.raw)

# Sum of missing observations
sum(is.na(election.raw))

# Count of unique observations for each variable
sapply(election.raw, function(x) length(unique(x)))
```
Election.raw has 31167 observations and 5 variables and no missing values.
There are 51 distinct values in election.raw which verfiies that all 50 states are included and a federal district. 

[2] Census data
```{r, results = FALSE}
# Dimension of census
dim(census)

# Sum of missing observations
sum(is.na(census))

# Count of unique observations in County
length(unique(census$County))
```
Census has 3220 observations and 37 variables and there is one missing value. 
There are 1955 disntinct values in in county in census, while there are 2825 isntinct values in in county in eleection.raw. There are substantially more distict values in election.raw, from looking at the data it seems that this is the result ofstates having counties with the same name (such as Ward 1, Ward 2, ... , Ward 8 from census). This is because in election.raw there are counties with same names in different states.

Data Wrangling
[3] Constructing aggregated data sets from election.raw data
```{r}
# Keep the county-level data as it is in election.raw

# Create a state-level summary into a election.state
election.state <- aggregate(votes ~ candidate + state, election.raw, sum)

# Create a federal-level summary into a election.total
election.total <- aggregate(votes ~ candidate, election.raw, sum)
```

[4] Bar chart of 2020 Presidential candidates
```{r, results = FALSE}
# How many named presidential candidates were there in the 2020 election?
length(unique(election.total$candidate))

```
There were 38 presidential candidates in the 2020 election.
```{r, fig.width=7}
# Bar chart of all votes received by each candidate
cex.names <- election.total$candidate
barplot(log(election.total$votes), main = "Votes Received by Each Candidate", 
        xlab="Candidate", ylab = "Number of Votes (log-scale)", 
        names = election.total$candidate, col = "light blue", cex.names = 0.8)
```


[5] Create data sets county.winner and state.winner by taking the candidate with the highest proportion of votes in both county level and state level
```{r,  message=FALSE}
# County winner
county.winner <- election.raw %>%
  group_by(county) %>%
  mutate(total = sum(votes), pct = votes/total) %>%
  top_n(1)

# State winner
state.winner <- election.state %>%
  group_by(state) %>%
  mutate(total = sum(votes), pct = votes/total) %>%
  top_n(1)
```




Visualization

[6] Draw county-level map
```{r}
counties = map_data("county")

ggplot(data = counties) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group),
               color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)
```

[7] Now color the map by the winning candidate for each state
```{r}
# Load data
states <- map_data("state")

# Change format of states to match state.winner
states <- states %>%
  mutate(state = str_to_title(region))

party_colors <- c("#CB454A", "#2E74C0") 

states.joint <- left_join(state.winner, states, by = c("state" = "state"), copy = TRUE)
# Create map
ggplot(data = states.joint) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group),
               color = "white", size = 0.1) + 
  coord_fixed(1.3) +
  scale_fill_manual(values = party_colors) +
  #guides(fill=FALSE) +
  ggtitle("Winning Candidate by State") 
  
```

[8] Color the map of the state of California by the winning candidate for each county
```{r}
# Load data
california <- map_data("county", "california")

# Change format of california subregion to match election.raw
california <- california %>%
  mutate(county = str_to_title(subregion))
# Join data
county.cal <- left_join(california, county.winner[county.winner$state == "California", ], 
                        by = c("county" = "county"))

# Create map
ggplot(data = county.cal) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group),
               color = "white", size = 0.1) + 
  coord_fixed(1.3) +
  scale_fill_manual(values = party_colors) +
  #guides(fill=FALSE) +
  ggtitle("Winning Candidate by County in California")
```

[9] (Open-ended) Create a visualization of your choice using census data
```{r}
# Create variable for percent of the population that is a minority 
census.9 <- census %>%
  mutate(census, Minority = Hispanic + Black + Native + Asian + Pacific)

# Load data
states <- map_data("state")
library(scales)

# Change format of states to match state.winner
states <- states %>%
  mutate(state = str_to_title(region))
states.joint.c <- left_join(census.9, states, by = c("State" = "state"), copy = TRUE)

ggplot(data = states.joint.c) + 
  geom_polygon( aes(x = long, y = lat, group = group, fill = Minority), color = "white", size = 0.1) +
  labs(title = "Percent of Population made up by Minorities") + labs(fill = "Percent") +
  scale_fill_gradient2(low = "#CB454A",
                      mid = scales::muted("purple"),
                      high = "#2E74C0",
                      breaks = c(0, 25, 50, 75),
                      midpoint = mean(30),
                      limits = c(min(0),max(80))) +
        labs(title = "Percent Of Population made up by Minorities") 
```
The graph shows percent of each state;s population made up my minority groups. As we can see the states that are more blue/ darker purple correlate with the states that Joe Biden won. However, there are many states that are blue/ purple that were won by Trump (including Texas, Floride, etc.) as there are states that are more red/ purple that were won by Biden (including Wisconsin, Michigan, etc.). From my analysis, it seems that Minority may not be a strong prediction factor as we can already see that high minority populations does have a strong effect in some states but not others. It will be intersting to see what factors played a role in states that Donald Trump won and have high minority populations. 

[10] Clean and aggregate census data
```{r}
# Filter out any rows with missing values
census.clean <- na.omit(census) 

# Convert {Men, Employed, VotingAgeCitizen} attributes to percentages, compute Minority attribute 
census.clean <- census.clean %>%
  mutate(census.clean, Men = (Men/TotalPop)*100, Employed = (Employed/TotalPop)*100, 
         VotingAgeCitizen = (VotingAgeCitizen/TotalPop)*100, 
         Minority = Hispanic + Black + Native + Asian + Pacific) 

# Remove these variables after creating Minority 
census.clean <- subset(census.clean, select = 
                         -c(Hispanic, Black, Native, Asian, Pacific, IncomeErr, IncomePerCap, 
                            IncomePerCapErr, Walk, PublicWork, Construction) )

# Checking collinearity 
#((census.clean$Women / census.clean$TotalPop) * 100) + census.clean$Men == 100 
  # Returns true for all values indicating that the columns Men and Women are perfectly colineared. 
census.clean <- subset(census.clean, select = -c(Women)) # remove variable Women
#census.clean$Minority + census.clean$White == 100
  # Returns false for majority of values indicating that the colunms White 
  # and Minority are not perfectly colineared

# Print the first 5 rows of census.clean
head(census.clean, 5)
```
I only removed the column Women because it is perfectly colineared with the column Men. I chose to only remove this variable because other columns do not have "pefect" colinearity or they have multilinearity. 


Dimensionality Reduction
[11] Run PCA for the cleaned county level census data (with State and County excluded)
```{r, fig.width=7, results = FALSE}
# Create subset excluding State and County
cc.exclude <- subset(census.clean, select = -c(State, County, CountyId))

pr.census = prcomp(cc.exclude, scale=TRUE)
#pr.census$center
#pr.census$scale
#pr.census$rotation
#pr.census$sdev

pc.county <- as.data.frame(pr.census$x[,1:2])

pc1.rotate <- sort(abs(pr.census$rotation[,1]), decreasing = TRUE)
kable(pc1.rotate %>% head(3))

kable(pr.census$rotation[,1])
```
I decided to exclude CountyId from the analysis here because it does not have any prediction power over voter choice and it is not a demogrpahic of a population and it is a redundant variable. I scaled the data during PCA because some variables are percentages while others are raw numbers meaning they are on different scales and need to be scaled in order for PCA to give us the most accurate output.
The three features with the largest absolute values of the first principal component Poverty, ChildPoverty, and Employed. 
TotalPop, Men, White, VotingAgeCitizen, Income, Professional, Transit, WorkAtHome, Employed, PrivateWork, Self Employed, and Family Work all have positive coefficents and the otheres have negative coeefficents. Negative coeffficents mean they are negatively correlated with the response. 
 
 
[12] Determine the number of minimum number of PCs needed to capture 90% of the variance for the analysis
```{r, results=FALSE}
# Standard deviation and variance explained by each principal component
pr.census$sdev
pr.var.cc=pr.census$sdev^2

#  Computing the proportion of variance explained by each principal component
pve.cc=pr.var.cc/sum(pr.var.cc)

cumsum(pve.cc) >= 0.9
```
The number of minimum number of PCs needed to capture 90% of the variance for the analysis is 11 Pcs. 

```{r, fig.width=7}
# Plotting the PVE explained by each component 
plot(pve.cc, xlab="Principal Component", cex = 2, 
     ylab="Proportion of Variance Explained", ylim=c(0,1),type='b', col = 'dark green')

# Plotting cumulative PVE
plot(cumsum(pve.cc), xlab="Principal Component ", cex = 2, 
     ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), 
     type='b', col = 'orange')
```


Clustering
[13] With census.clean (with State and County excluded), perform hierarchical clustering with complete linkage
```{r, fig.width=7, fig.height=11}
# Standardize the variables by subtracting mean and divided by standard deviation
std.census = scale(cc.exclude[, -c(1,2)], center=TRUE, scale=TRUE)

# Compute a euclidean distance matrix between the subjects
cc.dist <- dist(std.census)

# Agglomerative Hierarchical clutering
set.seed(1)
cc.hclust = hclust(cc.dist) # complete linkage
# Cut the tree to partition the observations into 10 clusters
cc.ctree <- cutree(cc.hclust, 10)

# Hierarchical clustering algorithm using the first 2 principal components from pc.county
pc.dist <- dist(pc.county)
set.seed(1)
pc.hclust = hclust(pc.dist)
pc.ctree <- cutree(pc.hclust, 10)

```
```{r, fig.width=7}
SantaBarbara <- which(census.clean$County == "Santa Barbara")

plot(scale(std.census), col=cc.ctree,
       main="Hierarchical Clustering on County", 
       sub="Clusters=10")
scalednumct <- as.data.frame(std.census)
abline(v = scalednumct$TotalPop[SantaBarbara], col = "blue")
 
plot(pc.county, col = pc.ctree, cex = 0.5,
     main="Hierarchical Clustering on County with  2 Principal Components", sub="clusters=10" )
abline(v = pc.county[SantaBarbara,], col = "blue")
```



Classification
```{r}
# we move all state and county names into lower-case
tmpwinner <- county.winner %>% ungroup %>%
  mutate_at(vars(state, county), tolower)

# we move all state and county names into lower-case
# we further remove suffixes of "county" and "parish"
tmpcensus <- census.clean %>% mutate_at(vars(State, County), tolower) %>%
  mutate(County = gsub(" county|  parish", "", County)) 

# we join the two datasets
election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

# drop levels of county winners if you haven't done so in previous parts
election.cl$candidate <- droplevels(election.cl$candidate)

## save meta information
election.meta <- election.cl %>% select(c(county, party, CountyId, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% select(-c(county, party, CountyId, state, votes, pct, total))
```

[14] Understand the code above. Why do we need to exclude the predictor party from election.cl?
We exclude the predictor part because is is not a useful tool is predicting election winners as it is simply indicating the part of the canidate. If the census dataset also had a predictor party which indicates the the percent of the population that identify as either democrat or republican then it may be a use predictor to keep in the data set. In this case, party will always predict the winning candidate correctly in out analysis because Donald Trump is always a rebuplican and Joe Biden is always a democrat. This will skew our analysis and effect how we interpret the power of other predictors.


```{r, echo=FALSE}
# Partition data into 80% training and 20% testing
set.seed(10) 
n <- nrow(election.cl)
idx.tr <- sample.int(n, 0.8*n) 
election.tr <- election.cl[idx.tr, ]
election.te <- election.cl[-idx.tr, ]
```

```{r, echo=FALSE}
# Define 10 cross-validation folds
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(election.tr), breaks=nfold, labels=FALSE))
```

```{r, echo=FALSE}
# Using the following error rate function. And the object records is used to record the classification performance of each method in the subsequent problems
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```


[15] Decision tree: train a decision tree by cv.tree()
```{r, results=FALSE}
# Fit and summarize the tree
tree.election = tree(candidate ~., data = election.cl)
summary(tree.election)

# Fit and summarize the tree for training data
tree.election.tr = tree(candidate ~., data = election.tr)
summary(tree.election.tr)
```
```{r, fig.width=7,}
# Visualize the trees
plot(tree.election)
text(tree.election, pretty = 0, cex = 0.5, col = 'purple')
title("Decision tree on Election Data", cex = 0.8)

plot(tree.election.tr)
text(tree.election.tr, pretty = 0, cex = 0.5, col = 'purple')
title("Decision tree on Election Data Training Set", cex = 0.8)
```
The decision tree tells us that the significant variables are White, TotalPop, Production, VotingAgeCitizen, Service, Professional, and Transit. The decision tree using the training data has similar predictors but also includes Men and Drive.

```{r, results=FALSE}
# Train decision tree and K-Fold cross validation
cv <- cv.tree(tree.election, FUN=prune.misclass, rand = folds)
cv.tr <- cv.tree(tree.election.tr, FUN=prune.misclass, rand = folds)

# Fine best size for building pruned tree
best.cv = min(cv$size[cv$dev == min(cv$dev)])
best.cv
best.cv.tr = min(cv.tr$size[cv.tr$dev == min(cv.tr$dev)])
best.cv.tr

# Prune tree.election
pt.cv = prune.misclass (tree.election, best=best.cv)
pt.cv.tr = prune.misclass (tree.election.tr, best=best.cv.tr)
```
By performing cross-validation on election.cl and the respective training data set and returning the best value of lambda, we find that the best lambda value for election.cl is 6 and for election.tr it is 7.

```{r,  fig.width=7}
# Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = 'purple', cex = 0.75)
title("Pruned tree of size 6")

plot(pt.cv.tr)
text(pt.cv.tr, pretty=0, col = 'purple', cex = 0.75)
title("Pruned tree of size 7 for Training Data")
```
The pruned decision tree tells us that significant variables are Transit, TotalPop, White, Employed, and Men. The pruned tree tells a story of voting behavior.

```{r}
# Calculating Train and Test error and saving errors to records object

# Predict on train set
pt.pred.tr = predict(pt.cv, election.tr, type = "class")
# Train error rate 
pt.train.error <- calc_error_rate(pt.pred.tr, election.tr$candidate)

# Predict on test set
pt.pred.te = predict(pt.cv.tr, election.te, type="class")
# Test error rate 
pt.test.error <- calc_error_rate(pt.pred.te, election.te$candidate)

# Saving to records object
records[1,1] = pt.train.error
records[1,2] = pt.test.error
```
The test error rate is slightly larger than the training error rate indicating that this model is fit well.

[16] Run a logistic regression to predict the winning candidate in each county
```{r, results=FALSE, warning=FALSE}
# Logistic regression model
glm.fit = glm(candidate ~ ., data = election.cl, family = binomial)
summary(glm.fit)

# Logistic regression model for trainiing data
glm.fit.train = glm(candidate ~ ., data = election.tr, family = binomial)
summary(glm.fit.train)
```
The logistic regression model tells us that the following variables are sifnificant: White, VotingAgeCitizen, Professional, Serice, Office, Production, Drive, Carpool, OtherTransp, Employed, PrivateWork, FamilyWork, and Unemployment. The variable White represent the percent of the total population which is ethnically white. It is very statistically significant and has a negative coeeficent which tells that the higher the population of people who are white the lower the chance that Joe Biden will win that county. On the other hand, the amount of voting age citizens and unemplyment rate will positively predict Joe Biden as the winner. This means the higher the number of voting age citizens or the higher the unemployment rate in a county the more likely Joe Biden is to win that county. 
This is extremely similar to the unpruned decision tree which shares many of the significant predictors. This is indicat that this model can be minimized to better predict outcomes or it may mean that the pruned tree cuts out too many predictors and underfits the data. 

```{r}
# Training error rate
lr.pred.train = predict(glm.fit, election.tr, type = "response")
glm.train = rep('Donald Trump', dim(election.tr)[1])
glm.train[lr.pred.train > 0.5] = "Joe Biden"
lr.train.error <- calc_error_rate(glm.train, election.tr$candidate)

# Test error rate
lr.pred.test = predict(glm.fit.train, election.te, type = "response")
glm.test = rep('Donald Trump', dim(election.te)[1])
glm.test[lr.pred.test > 0.5] = 'Joe Biden'
lr.test.error <- calc_error_rate(glm.test , election.te$candidate)

# Saving to records object
records[2,1] = lr.train.error
records[2,2] = lr.test.error
```
Again for the logistic regression model the test error rate is slightly higher than the training error rate indicating that the model is a good fit.

[17] LASSO logistic regression
```{r}
x = model.matrix(candidate~., data = election.cl)[,-1]
y = election.cl$candidate

x.train=x[idx.tr,]
y.train=election.cl[idx.tr,]$candidate
x.test=x[-idx.tr,]
y.test=election.cl[-idx.tr,]$candidate
```

```{r}
# Fitting a lasso model to predict Candidate
lambda = seq(1, 50) * 1e-4
set.seed(10)
cv.out.lasso = cv.glmnet(x, y, lamda = lamba, alpha = 1, family=binomial)
plot(cv.out.lasso)
abline(v = log(cv.out.lasso$lambda.min), col="red", lwd=3, lty=2)

# Fitting a lasso model to the training set to predict Candidate
cv.out.lasso.tr = cv.glmnet(x.train, y.train, lamda = lamba, alpha = 1, family=binomial)

```
```{r, echo=FALSE}
# Cross-validation to Choose the Best Tuning Parameter
bestlam = cv.out.lasso$lambda.min

# Coeeficients of lasso model with optimal valuse of lambda
out1 = glmnet(x, y, alpha=1, lambda = lambda, family=binomial)
lasso.coef = predict(out1, type="coefficients", s = bestlam)[1:24,]
kable(lasso.coef)
```
The optimal value of lambda is 0.001506243 in cross validation. The non-zero coefficients in the LASSO regression for the optimal value of lambda are TotalPop, Men, White, VotingAgeCitizen, Poverty, Professional,Office, Production, Drive, Carpool, OtherTransp, MeanCommute, Employed, PrivateWork, SelfEmployed, FamilyWork, and Unemployment. This means 5 of the 23  coefficient estimates are exactly zero. This is interesting because the model removes 5 variables by concludign that they are not significant for prediction county winners. The largest (absolute value) coeefficents line up with the the significant variables in the unpenalized logistic regression, however, the the variable FamilyWork has the most negative coefficent (and greatest overall) in the lasso model when it is only slightly significant in the other model. This is most likely the result of the reduction of variables in the lasso model which resultd in other variables becoming more significant.



```{r}
lasso.mod <- glmnet(x, y, alpha=1, lambda=lambda, family = binomial)
lasso.mod.train <- glmnet(x.train, y.train, alpha=1, lambda=lambda, family = binomial)

# Training error for lasso model
lasso.pred.train = predict(lasso.mod, s = bestlam, newx = x[idx.tr,])
lasso.train = rep('Donald Trump', dim(election.tr)[1])
lasso.train[lasso.pred.train > 0.5] = 'Joe Biden'
lasso.train.error <- calc_error_rate(lasso.train , election.tr$candidate)

# Test error for lasso model
lasso.pred.test = predict(lasso.mod.train, s = bestlam, newx = x[-idx.tr,])
lasso.test = rep('Donald Trump', dim(election.te)[1])
lasso.test[lasso.pred.test > 0.5] = 'Joe Biden'
lasso.test.error <- calc_error_rate(lasso.test , election.te$candidate)

# Saving to records object
records[3,1] = lasso.train.error
records[3,2] = lasso.test.error
```
The LASSO model has a higher test error rate than training error rate indicating thee model is a good fit.
```{r, echo=FALSE}
kable(records)
```
From the records matrix we can see that all classificastion models are fit well. The logistic regression model has the lowest test and training error rates which tells us that is that it will predict presidential winners by county with the most accuracy based on the given predictors. 

[18] Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data
```{r}
# ROC curve for decision tree
#pred.pt = prediction(as.matrix(pt.pred.te), election.te$candidate)
#perf.pt = performance(pred.pt, measure="tpr", x.measure="fpr")
#plot(perf.pt, col=2, lwd=3, main="ROC Curve for Decision Tree")
#abline(0,1)
```
I was not able to create the ROC curve for the pruned tree because I had trouble making it a continuous variable.

```{r, fig.width=7, echo=FALSE}
# ROC curve for logistic regression model
pred.lr = prediction(lr.pred.test, election.te$candidate)
perf.lr = performance(pred.lr, measure="tpr", x.measure="fpr")
plot(perf.lr, col="pink", lwd=3, main="ROC Curves using predictions on the test data")
abline(0,1)

# ROC curve for LASSO logistic regression model
pred.lasso = prediction(lasso.pred.test, election.te$candidate)
perf.lasso = performance(pred.lasso, measure="tpr", x.measure="fpr")
plot(perf.lasso, col="purple", lwd=3, main="ROC Curve for Logistic Regression Model", add = TRUE)
abline(0,1)

legend("bottom",
       legend=c("Decision Tree", "Logistic Regression Model", "LASSO Logistic Regression Model"),
       col=c("orange", "pink", "purple"),
       lwd=4, cex =0.5, xpd = TRUE, horiz = TRUE)
```
The ROC curve for the logistic regression model and the LASSO logistic model are are almost identical showing that they are suited for answering similar kinds of questions regaurding the data. The curves also tells us that these two type of classification may be somewhat redundant for preliminary analysis on this model.


[19] Explore additional classification methods

Support Vector Machine
```{r}
# SVM fir
svmfit = svm(candidate ~ ., data = election.cl, kernel="radial", cost=10, scale=FALSE)
svm.pred.tr = predict(svmfit, election.tr)
svm.train.error <- calc_error_rate(svm.pred.tr, election.te$candidate)

#SVM fir on training data
svmfit.train = svm(candidate ~ ., data = election.tr, kernel="radial", cost=10, scale=FALSE)
svm.pred.te = predict(svmfit.train, election.te)
svm.test.error <- calc_error_rate(svm.pred.te, election.te$candidate)
```
```{r}
set.seed(1)
# Cross validation to find best parameters
tune.out = tune(svm, candidate ~ ., data=election.cl, kernel="radial",
              ranges=list(cost=c(0.001,0.01, 0.1, 1, 10, 100)))
summary(tune.out)$"best.parameters"

bestmod = tune.out$best.model
summary(bestmod)

svm.pred.tr = predict(bestmod, newdata = election.tr)
svm.train.error <- calc_error_rate(svm.pred.tr, election.tr$candidate)

```
```{r}
# Cross validation to find best parameters on training data
tune.out.tr=tune(svm, candidate ~ ., data=election.tr, kernel="radial",
              ranges=list(cost=c(0.001,0.01, 0.1, 1, 10, 100)))
summary(tune.out.tr)$"best.parameters"

bestmod.tr = tune.out.tr$best.model
summary(bestmod.tr)

svm.pred.te = predict(bestmod.tr, newdata = election.te)
svm.test.error <- calc_error_rate(svm.pred.te, election.te$candidate)
```
The best lambda value for both models is 1.

```{r, echo=FALSE}
comparison = matrix(NA, nrow=5, ncol=2)
colnames(comparison) = c("train.error","test.error")
rownames(comparison) = c("tree","logistic","lasso", "svm", "boost")

comparison[1,1] = pt.train.error
comparison[1,2] = pt.test.error
comparison[2,1] = lr.train.error
comparison[2,2] = lr.test.error
comparison[3,1] = lasso.train.error
comparison[3,2] = lasso.test.error
comparison[4,1] = svm.train.error
comparison[4,2] = svm.test.error

comparison
```
I chose to explore the support vector machine classification method because it is primarily intended for binaray classification and the predictors of election.cl are binary. Moreoever, I used a radial kernel because in earlier analysis it is clear that the boundaries between classes is not always linear. Intuitively, with this data thre will be values that cross the boundries because demogrphics can only predict voting choice to an extent and there will always be outliers; making the radial kernel more fitting. From the comparison matrix I created shows that the SVM model returns the lowest training and test error, showing that it most effectively predicts which candidate will win based on the predictors. However the test error is significantly higher than the training error. This is most likely the result of partitioning the data into 80% training and 20% testing, which may have lead to too many "hard" cases oin the training data and many "easy" cases in the test set. This model suffers from overfitting; I attempted to change the kernel from radial to linear, but that resulted in a higher training and test error with the test error still being higher. Although the model error rates indicate that the model is over fitted the cost is 1 for both models and it produces the lowest training and test errors. 


Boosting
```{r}
set.seed(1)
# Fit boosting model using 6 trees from earlier cross validation
boost.election = gbm(ifelse(candidate=="Joe Biden",1,0) ~ ., data = election.cl, distribution = "bernoulli", n.trees = 500,
                     interaction.depth = 2,shrinkage = 0.01)


## Fit boosting model on training data using 7 trees from earlier cross validation
boost.election.tr = gbm(ifelse(candidate=="Joe Biden",1,0) ~ ., data = election.tr, distribution = "bernoulli", n.trees = 500,
                     interaction.depth = 2,shrinkage = 0.01)
```

```{r, message=FALSE}
pred.boost.tr = predict(boost.election, newdata = election.tr, type = 'response')
pred.boost.tr = ifelse(pred.boost.tr > 0.5, 1, 0)
boost.train.error <- calc_error_rate(pred.boost.tr, ifelse(election.tr$candidate=="Joe Biden",1,0))


boost.pred.te = predict(boost.election.tr, newdata = election.te, type = 'response')
boost.pred.te = ifelse(boost.pred.te > 0.5, 1, 0)
boost.test.error <- calc_error_rate(boost.pred.te, ifelse(election.te$candidate=="Joe Biden",1,0))

```

```{r, echo=FALSE}
comparison[5,1] = boost.train.error
comparison[5,2] = boost.test.error
kable(comparison)
```
Additionally, I explored using a boosting tree as a classification method. I first tried to use the number of trees calculated by cross validation in my earlier anyalysis, but found that this resulted in huge error rates so I tested using n.tree = 100, n.tree = 500, and n.tree = 1000. Using 500 trees resulted in significantly lower test and training errors compared to using 100 trees, but the error rates for 500 and 1000 trees did not differ much. I chose 500 trees because I wanted to minimize the about of trees being used to avoid overfitting. The booting model resultedin similar error rates compared to the logistic regression model, however it does have a slighly greater differece between the two error rates which means it might be overfitted compared to the logistic regression model. From my analysis I believe that the boosting model can more accurately predict presidential winners compared to the decision tree and lasso model.

[20] Exploratory analysis of Texas and Georgia
Prior to the 2020 election there were hope that both Georgia and Texas would turn blue; however Georgia turned blue by a slim margin and Texas stayed red. I will look at the winning candidates in the counties of both states and then look at some other factors that may have contributed to this. 
```{r, fig.width=7, fig.height=11, echo=FALSE}
# Create variable for percent of the population that is a minority 
census.min <- census %>%
  mutate(census, Minority = Hispanic + Black + Native + Asian + Pacific)

# Load data
texas <- map_data("county", "texas")

# Change format of california subregion to match election.raw
texas <- texas %>%
  mutate(county = str_to_title(subregion))
# Join data
county.tx <- left_join(texas, county.winner[county.winner$state == "Texas", ], by = c("county" = "county"))

# Create map
texas.plot <- ggplot(data = county.tx) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group),
               color = "white", size = 0.1) + 
  coord_fixed(1.3) +
  scale_fill_manual(values = party_colors) +
  #guides(fill=FALSE) +
  ggtitle("Winning Candidate by County in Texas")

# Load data
georgia <- map_data("county", "georgia")

# Change format of california subregion to match election.raw
georgia <- georgia %>%
  mutate(county = str_to_title(subregion))

# Join data
county.ga <- left_join(georgia, county.winner[county.winner$state == "Georgia", ], by = c("county" = "county"))

# Create map
ga.plot <- ggplot(data = county.ga) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group),
               color = "white", size = 0.1) + 
  coord_fixed(1.3) +
  scale_fill_manual(values = party_colors) +
  #guides(fill=FALSE) +
  ggtitle("Winning Candidate by County in Georgia")

```

```{r, fig.width=7, fig.height=11, echo=FALSE}
# Change format of county to match census data
texas.c <- texas %>%
  mutate(county = paste(county, "County", sep=" "))

# Join data
county.tx.min <- left_join(texas.c, census.min[census.min$State == "Texas",], by = c("county" = "County"))

tx.min <- ggplot(data = county.tx.min) + 
  geom_polygon( aes(x = long, y = lat, group = group, fill = Minority), color = "white", size = 0.1) +
  labs(title = "Percent of Population made up by Minorities") + labs(fill = "Percent") +
  scale_fill_gradient2(low = "#CB454A",
                      mid = scales::muted("purple"),
                      high = "#2E74C0",
                      breaks = c(0, 25, 50, 75),
                      midpoint = mean(30),
                      limits = c(min(0),max(80))) +
        labs(title = "Percent of Population made up by Minorities") 


# Change format of county to match census data
georgia.c <- georgia %>%
  mutate(county = paste(county, "County", sep=" ")) 

# Join data
county.ga.min <- left_join(census.min[census.min$State == "Georgia",], georgia.c, by = c("County" = "county"))


ga.min <- ggplot(data = county.ga.min) + 
  geom_polygon( aes(x = long, y = lat, group = group, fill = Minority), color = "white", size = 0.1) +
  labs(title = "Percent of Population made up by Minorities") + labs(fill = "Percent") +
  scale_fill_gradient2(low = "#CB454A",
                      mid = scales::muted("purple"),
                      high = "#2E74C0",
                      breaks = c(0, 25, 50, 75),
                      midpoint = mean(30),
                      limits = c(min(0),max(80))) +
        labs(title = "Percent of Population made up by Minorities") 

tx.fam <- ggplot(data = county.tx.min) + 
  geom_polygon( aes(x = long, y = lat, group = group, fill = White), color = "white", size = 0.1) +
  labs(title = "Percent of Population made up by Minorities") + labs(fill = "Percent") +
  scale_fill_gradient2(low = "#2E74C0",
                      mid = scales::muted("purple"),
                      high = "#CB454A",
                      breaks = c(0, 25, 50, 75),
                      midpoint = 30,
                      limits = c(min(0),max(80))) +
        labs(title = "Percent of Population White") 


# Change format of county to match census data
georgia.c <- georgia %>%
  mutate(county = paste(county, "County", sep=" ")) 

# Join data
county.ga.min <- left_join(census.min[census.min$State == "Georgia",], georgia.c, by = c("County" = "county"))


ga.fam <- ggplot(data = county.ga.min) + 
  geom_polygon( aes(x = long, y = lat, group = group, fill = White), color = "white", size = 0.1) +
  labs(title = "Percent of Population made up by Minorities") + labs(fill = "Percent") +
  scale_fill_gradient2(low = "#2E74C0",
                      mid = scales::muted("purple"),
                      high = "#CB454A",
                      midpoint = 40,
                      limits = c(min(0),max(80))) +
        labs(title = "Percent of Population White") 

grid.arrange(texas.plot, ga.plot, tx.min, ga.min, tx.fam, ga.fam, nrow = 3, ncol = 2)
```
To better understand thr role of minority populations and white populations I created six maps to look at the possible prediction power. I used the variables Minority and white becausee they help fill in missing data that shades the states fully. From these maps we can clearly see that the counties that Joe Biden won are shaded in purple/blue on the bottom four maps. Thisindicates either a high minority population or low white population. However, it is difficult to understand how Georgia turned blue and Texas stayed read from these maps. It will take futher investigation and analysis to understand this. I would first start by collecting data on the 2016 election so I can create models to compare the two. I think if it difficult to predict these states in each election because there is a large minority population but it seems to be match by a white population. Among other factors, I believe that these states do not have high rates or percentages of any of the demogrpahics in pur analysis and threfore predicting because difficult without an outstanding statistics to guide.
Moreover, in the above anlysis Minority is not a significant predictor however, from the graphs above I believe that it can be if paired with the variabole White. I chose not to exclude the White or Minority in my analysis early because I found that they were not perfectly collineared, however, after analysis Minority could have been excluded as it was not significant. It would have been sufficent to only include White. 


[21] Interpret and discuss any overall insights gained in this analysis and possible explanations.
Political polling can be a a very difficuly task. One must remove all bias and look at trends within the data to accurately understand and predict political winners. Predictors can be tricky as we can never be sure how a person will vote no matter the amount of information we may have of their daily life. It takes a closer look into how county, state, and federal demographics play a role in candidate picking to be able to predict elections. My overall analysis and conclusion is that no one predictor is powerful enough to predict election results alone. 

Overall, I found it extremely intersting which demogrpahics play the largest role in predicting presidential election outcome; much of the analysis was surprisng and vary interesting.Firstly, in all of the classification models in this report minority population did not have a significant impact on the winniing candiate but intuitively one would think that this would have at least some impact. In the lasso model is was given a coeefficient of 0, it was not included in the decision tree, and the logistic model deemed it insignificant. However, after I created the graphs above I belive this may be becaused it is collineared with White. I think this may have skewed my data in some ways leading to some minor errors. I do think it is a bit strange that Transit was a significant variable in multiply models; mode of transportation is an unusual way to predict someone's voting preferences. Other than Transit, the other significant variabes seem resonable.

To strenthen the prediction power, this data should be paired with election and census data from the 2016 election. Other factors like age, education level, and sexual orientation would also strethen this analysis. The analysis that we have here works on a larger spectrum, but I feel it may not be as accurate at predicting on a smaller scale. 







